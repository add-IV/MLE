{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "####################################\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = self.input * (self.input > 0) # your code here\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        downstream_gradient = upstream_gradient * (self.input > 0) # chain rule\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = np.exp(input) / np.sum(np.exp(input), axis = 1, keepdims=True) # definition of the softmax\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        downstream_gradient = predicted_posteriors - true_labels # as derived in the lecture\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.normal(0, 0.01, (n_inputs, n_outputs)) #initialize the weights and biases with small random values\n",
    "        self.b = np.random.normal(0, 0.01, (1, n_outputs)) # your code here\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = np.dot(self.input, self.B) + self.b # your code here\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = upstream_gradient.mean(axis=0, keepdims=True)  #bias is the same for the whole batch\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient) # derivative\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T) # chain rule\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n",
    "\n",
    "####################################\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "        #print(X.shape)\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        grad = self.layers[-1].backward(predicted_posteriors, true_classes) #last layer backpropagation\n",
    "        for layer in reversed(self.layers[:-1]): # loop through the remaining layers starting at the end\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            #print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set training/test set size\n",
    "N = 2000\n",
    "\n",
    "# create training and test data\n",
    "X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "n_features = 2\n",
    "n_classes  = 2\n",
    "\n",
    "# standardize features to be in [-1, 1]\n",
    "offset  = X_train.min(axis=0)\n",
    "scaling = X_train.max(axis=0) - offset\n",
    "X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "# one-hot encode labels so the network can actually work with them\n",
    "Y_train_oh = np.eye(n_classes)[Y_train]\n",
    "Y_test_oh = np.eye(n_classes)[Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Size: 5\n",
      "Layer Size: 2,\tError Rate: 0.5\n",
      "Layer Size: 3,\tError Rate: 0.1625\n",
      "Layer Size: 5,\tError Rate: 0.1155\n",
      "Layer Size: 30,\tError Rate: 0.1265\n",
      "Epoch Size: 10\n",
      "Layer Size: 2,\tError Rate: 0.5\n",
      "Layer Size: 3,\tError Rate: 0.1295\n",
      "Layer Size: 5,\tError Rate: 0.1145\n",
      "Layer Size: 30,\tError Rate: 0.128\n",
      "Epoch Size: 100\n",
      "Layer Size: 2,\tError Rate: 0.5\n",
      "Layer Size: 3,\tError Rate: 0.5\n",
      "Layer Size: 5,\tError Rate: 0.1095\n",
      "Layer Size: 30,\tError Rate: 0.1255\n"
     ]
    }
   ],
   "source": [
    "#loop trough the layer sizes given in the exercise and store the error rates:\n",
    "error_rates = []\n",
    "epoch_sizes = [5, 10, 100]\n",
    "neuron_sizes = [2, 3, 5, 30]\n",
    "for n_epochs in epoch_sizes:\n",
    "    error_rates_for_epoch_size = []\n",
    "    print(f\"Epoch Size: {n_epochs}\")\n",
    "    for layer_neurons in neuron_sizes:\n",
    "\n",
    "        layer_sizes = [layer_neurons, layer_neurons, n_classes]\n",
    "        \n",
    "        n_epochs = 5\n",
    "        batch_size = 200\n",
    "        learning_rate = 0.05\n",
    "        # create network\n",
    "        network = MLP(n_features, layer_sizes)\n",
    "\n",
    "        # train\n",
    "        network.train(X_train, Y_train_oh, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "        # test\n",
    "        predicted_posteriors = network.forward(X_test)\n",
    "        # determine class predictions from posteriors by winner-takes-all rule\n",
    "        predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "        \n",
    "\n",
    "        # compute and output the error rate of predicted_classes\n",
    "        error_rate =np.mean(predicted_classes != Y_test) # your code here\n",
    "        print(f\"Layer Size: {layer_neurons},\\tError Rate: {error_rate}\")\n",
    "        error_rates_for_epoch_size.append(error_rate)\n",
    "    error_rates.append(error_rates_for_epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates:\n",
      "╒═════╤═════╤════════╤════════╤════════╕\n",
      "│     │   2 │      3 │      5 │     30 │\n",
      "╞═════╪═════╪════════╪════════╪════════╡\n",
      "│   5 │ 0.5 │ 0.1625 │ 0.1155 │ 0.1265 │\n",
      "├─────┼─────┼────────┼────────┼────────┤\n",
      "│  10 │ 0.5 │ 0.1295 │ 0.1145 │ 0.128  │\n",
      "├─────┼─────┼────────┼────────┼────────┤\n",
      "│ 100 │ 0.5 │ 0.5    │ 0.1095 │ 0.1255 │\n",
      "╘═════╧═════╧════════╧════════╧════════╛\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "error_rates_tabular = [[\"\"] + neuron_sizes] + [[epoch_sizes[i]] + error_rates[i] for i in range(len(error_rates))]\n",
    "print(\"Error rates:\")\n",
    "print(tabulate.tabulate(error_rates_tabular, headers=\"firstrow\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For few epochs, the larger networks perform better, while for many epochs, the large networks perform worse due to overtraining. In general though, the performance is highly dependant on the random batches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
